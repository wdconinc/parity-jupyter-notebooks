{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finding Best Regression Sets in Single Pass Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdconinc/parity-jupyter-notebooks/blob/master/Finding%20Best%20Regression%20Sets%20in%20Single%20Pass%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RzTChW53KAdV"
      },
      "cell_type": "markdown",
      "source": [
        "# Finding Best Linear Regresssion Sets In Single Pass Analysis"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UJyolq12DYm7"
      },
      "cell_type": "markdown",
      "source": [
        "## Preamble"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VqNouFd1J7gF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RnY_R32NKLtg"
      },
      "cell_type": "markdown",
      "source": [
        "## Create Test Data Set With Random Correlations"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QnUy6aqxDhPw"
      },
      "cell_type": "markdown",
      "source": [
        "### Choose Dimensionality"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DIAHbY9VKPJ4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# number of dependent variables\n",
        "M = np.random.randint(10, 15)\n",
        "# number of independent variables\n",
        "N = np.random.randint(10, 15)\n",
        "# number of larger correlations between dependent and independent\n",
        "K = np.random.randint(2, min(M,N))\n",
        "\n",
        "print(\"Using %d independent variables with %d large correlations, %d dependent variables\" % (N,K,M))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Lu8MKkq8WZyQ"
      },
      "cell_type": "markdown",
      "source": [
        "### Create Correlation Matrix"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VaL-HymADzbn"
      },
      "cell_type": "markdown",
      "source": [
        "If we start from $Z$, a column of independently and normally distributed random variables, then we can construct $X$, a column of correlated and normally distributed random variables with covariance matrix $\\Sigma$ as follows:\n",
        "- find the Cholesky decomposition of $\\Sigma$ (positive-definite) into an upper triangular $C$:\n",
        "$$ \\Sigma = C^T C $$\n",
        "- apply Cholesky matrix to $Z$:\n",
        "$$ X = C^T Z $$\n",
        "\n",
        "Reference: http://www.columbia.edu/~mh2078/MonteCarlo/MCS_Generate_RVars.pdf"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y0lTEbx2Wb9n"
      },
      "cell_type": "markdown",
      "source": [
        "We can also write $\\Sigma$ in terms of the matrix of correlation coefficients $\\mathrm{Corr}[X]$, where the diagonal elements will be ones and the off-diagonal elements between -1 and +1:\n",
        "\\begin{align}\n",
        "\\Sigma & = Cov[X] \\\\ \n",
        "& = Diag[Var(X)]^\\frac{1}{2} \\, Corr[X] \\, Diag[Var(X)]^\\frac{1}{2} \\\\\n",
        "& = Diag[\\Sigma]^\\frac{1}{2} \\, Corr[X] \\, Diag[\\Sigma]^\\frac{1}{2} \\\\\n",
        "& = \\sigma \\, R \\, \\sigma\n",
        "\\end{align}\n",
        "\n",
        "We will use the [covariance matrix in block form](https://en.wikipedia.org/wiki/Covariance_matrix#Block_matrices).\n",
        "$$ \\Sigma = \\Big[ \\begin{array}{cc} \\Sigma_{XX} & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_{YY} \\end{array}\\Big] $$\n",
        "and\n",
        "$$ R = \\Big[ \\begin{array}{cc} R_{XX} & R_{XY} \\\\ R_{YX} & R_{YY} \\end{array}\\Big] $$\n",
        "with $X$ the so-called independent variables and $Y$ the so-called dependent variables."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uKPJv3RMXbpm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Construct diagonal of Sigma\n",
        "diag0 = np.diag(0.1 + np.random.randn(M + N) * 0.01)\n",
        "\n",
        "# Construct random correlation matrix\n",
        "corr0 = np.random.randn(N+M, N+M) * 0.01 # normal at 1% level\n",
        "corr0[:N,:N] = np.random.randn(N, N) * 0.1 # normal at 10% level\n",
        "corr0[N:,N:] = np.random.randn(M, M) * 0.1 # normal at 10% level\n",
        "corr0[:K,:K] = (np.random.rand(K, K) * 2 - 1) # uniform between -1 and +1\n",
        "corr0[:K,N:] = (np.random.rand(K, M) * 2 - 1) # uniform between -1 and +1\n",
        "np.fill_diagonal(corr0, 1) # diagonal is 1\n",
        "corr0 = (corr0 + corr0.T) / 2 # symmetrize\n",
        "corr0 = corr0 @ corr0.T # positive definite\n",
        "\n",
        "# Make sure correlation matrix is normalized\n",
        "norm0 = np.diag(np.sqrt(np.diag(corr0))**-1)\n",
        "corr0 = norm0 @ corr0 @ norm0\n",
        "corr0 = (corr0 + corr0.T) / 2.0\n",
        "\n",
        "# Determine covariance\n",
        "cov0 = diag0 @ corr0 @ diag0\n",
        "\n",
        "# Plot correlation matrix\n",
        "fig,ax = plt.subplots(1,2)\n",
        "ax[0].matshow(corr0, cmap = \"hot_r\")\n",
        "\n",
        "# Cholesky decomposition (returns *lower* triangular matrix)\n",
        "C = np.linalg.cholesky(cov0)\n",
        "ax[1].matshow(C, cmap = \"hot_r\") \n",
        "np.testing.assert_allclose(C @ C.T, cov0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HERFBUj2Vgfo"
      },
      "cell_type": "markdown",
      "source": [
        "### Create Correlated Data Set"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tuT2gHXhKa9S",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of measurements\n",
        "n = 1000000\n",
        "\n",
        "# Create X and Y\n",
        "Z = np.random.randn(N + M, n)\n",
        "XY = C @ Z\n",
        "X = XY[:N]\n",
        "Y = XY[N:]\n",
        "\n",
        "# Plot some data\n",
        "plt.matshow(XY[:,:200], cmap = \"hot_r\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fM_1JtxlRzbA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cov = np.cov(XY)\n",
        "corr = np.corrcoef(XY)\n",
        "\n",
        "fig, ax = plt.subplots(2,3)\n",
        "ax[0,0].matshow(cov, cmap = \"hot_r\")\n",
        "ax[0,1].matshow(cov0, cmap = \"hot_r\")\n",
        "ax[0,2].matshow(cov0 - cov, cmap = \"hot_r\")\n",
        "np.testing.assert_allclose(cov0, cov, atol = 0.2 * np.average(cov))\n",
        "\n",
        "ax[1,0].matshow(corr, cmap = \"hot_r\")\n",
        "ax[1,1].matshow(corr0, cmap = \"hot_r\")\n",
        "ax[1,2].matshow(corr0 - corr, cmap = \"hot_r\")\n",
        "np.testing.assert_allclose(corr0, corr, atol = 0.2 * np.average(corr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dlkJvPw8XEh5"
      },
      "cell_type": "markdown",
      "source": [
        "## Determine Summary Statistics Before Correction"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LWfJZLXJS-D7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "means = np.mean(XY, axis = 1)\n",
        "dmeans = np.sqrt(vars) / np.sqrt(n)\n",
        "vars = np.var(XY, axis = 1)\n",
        "\n",
        "# Plot means and variances\n",
        "fig,ax = plt.subplots(1,2)\n",
        "ax[0].errorbar(range(len(means)), means, dmeans, fmt = 'o')\n",
        "ax[1].errorbar(range(len(vars)), vars, fmt = 'o')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jZtxBhBtXJbz"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate Corrected Values"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2F92RFnuFS6U"
      },
      "cell_type": "markdown",
      "source": [
        "#### Model\n",
        "We hypothesize that the observed values for $Y$ are related to the observed values for $X$ by\n",
        "$$ Y = Y' + X A_{XY} + \\epsilon $$\n",
        "in the presence of the normally distribute noise $\\epsilon = \\mathcal{N}(0, \\sigma_Y^2 I)$. \n",
        "\n",
        "We wish to determine the underlying $E[Y']$ and $Cov[XY']$ from the observed $E[Y]$ and $Cov[XY]$. We wish to determine an estimator $\\hat{A}_{XY}$ and a statistical test to determine how many parameters $p$ in $X$ should be included (i.e. for which $p$ is the null hypothesis of the observed dependence allowed for $p-1$ parameters excluded at a certain probability)."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T2slq3cyFVWn"
      },
      "cell_type": "markdown",
      "source": [
        "#### Covariance and Correlation Matrices\n",
        "The regression corrections are determined for the observed correlation matrix $R$, covariance matrix $\\Sigma$, and diagonal matrix $\\sigma$ of standard deviations.\n",
        "\n",
        "$$ \\Sigma = \\Big[ \\begin{array}{cc} \\Sigma_{XX} & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_{YY} \\end{array}\\Big] = \\Big[ \\begin{array}{cc} \\sigma_X & 0 \\\\ 0 & \\sigma_Y\\end{array} \\Big] \\Big[ \\begin{array}{cc} R_{XX} & R_{XY} \\\\ R_{YX} & R_{YY} \\end{array}\\Big] \\Big[ \\begin{array}{cc} \\sigma_X & 0 \\\\ 0 & \\sigma_Y\\end{array} \\Big] = \\sigma R \\sigma $$\n",
        "\n",
        "with the indepedent and dependent variable covariance and correlation matrices\n",
        "$$ \\Sigma_{XX} = \\sigma_X R_{XX} \\sigma_X $$\n",
        "$$ \\Sigma_{XY} = \\sigma_X R_{XY} \\sigma_Y $$\n",
        "$$ \\Sigma_{YY} = \\sigma_Y R_{YY} \\sigma_Y $$\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ki_zIDR96Pnj"
      },
      "cell_type": "markdown",
      "source": [
        "#### Removing Correlations Between $Y'$ and $X$\n",
        "We wish to determine $A_{XY}$ such that the corrected dependent variables\n",
        "$$ Y' = Y - X A_{XY} $$\n",
        "are uncorrelated with $X$. This occurs when their cross covariance matrix is zero:\n",
        "$$ Cov[X,Y'] = E[(X - E[X])(Y' - E[Y'])] = 0 $$\n",
        "\n",
        "We can expand this and solve for $A_{XY}$:\n",
        "\\begin{align}\n",
        "E[(X - E[X])(Y - XA_{XY} - E[Y] + E[X] A_{XY})] & = 0 \\\\\n",
        "E[(X - E[X])(Y - E[Y])] - E[(X - E[X])(X - E[X])] A_{XY} & = 0 \\\\\n",
        "Cov[X,Y] - Var[X] A_{XY} & = 0 \\\\\n",
        "\\Sigma_{XY} - \\Sigma_{XX} A_{XY} & = 0\n",
        "\\end{align}\n",
        "\n",
        "The solution occurs for a specific $\\hat{A}_{XY}$:\n",
        "$$ \\hat{A}_{XY} = \\Sigma_{XX}^{-1} \\Sigma_{XY} $$\n",
        "which exists when the inverse of $\\Sigma_{XX}$ exists, or when there are no linearly related variables in the set of independent variables $X$.\n",
        "\n",
        "We can write this in terms of correlation matrices as well (this is the notation that is used in Qweak linear regression):\n",
        "$$ \\hat{A}_{XY} = \\Sigma_{XX}^{-1} \\Sigma_{XY} = (\\sigma_X^{-1} R_{XX}^{-1} \\sigma_X^{-1}) (\\sigma_X R_{XY} \\sigma_Y) =\\sigma_X^{-1} R_{XX}^{-1} R_{XY} \\sigma_Y $$\n",
        "\n",
        "In this case the mean and variance just become\n",
        "\\begin{align}\n",
        "E[Y'] & = E[Y] - E[X] \\hat{A}_{XY} \\\\\n",
        "Var[Y'] & = E[(Y' - E[Y'])^2] \\\\ & = E[(Y - X \\hat{A}_{XY} - E[Y] + E[X] \\hat{A}_{XY})^2] \\\\ & = E[(Y - E[Y]) - (X - E[X])\\hat{A}_{XY})^2] \\\\ & = Cov[Y] + Cov[X\\hat{A}_{XY}] - 2 Cov[X,Y] \\hat{A}_{XY} \\\\ & = Cov[Y] + \\hat{A}_{YX} Cov[X] \\hat{A}_{XY} - 2 Cov[X,Y] \\hat{A}_{XY} \\\\\n",
        "& = \\Sigma_{YY} + \\hat{A}_{YX} \\Sigma_{XX} \\hat{A}_{XY} - 2 \\Sigma_{YX} \\hat{A}_{XY}\n",
        "\\end{align}\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fG2yJkpTFhxe"
      },
      "cell_type": "markdown",
      "source": [
        "#### Uncertainty on Regression Coefficients\n",
        "To obtain the uncertainty on the regression coefficients, we introduce a normally distributed noise variable $\\epsilon = \\mathcal{N}(0,\\sigma^2)$ on $Y$ such that:\n",
        "$$ Y = X A_{XY} + \\epsilon = X A_{XY} +\\mathcal{N}(0,\\sigma_Y^2) $$\n",
        "\n",
        "If we write\n",
        "\\begin{align}\n",
        "\\hat{A}_{XY} & = \\Sigma_{XX}^{-1} \\Sigma_{XY} \\\\\n",
        "& = E[(X^T X)^{-1}] E[(X^T Y)] \\\\\n",
        "& = E[(X^T X)^{-1}] E[(X^T (X A_{XY} + \\epsilon)] \\\\\n",
        "& = A_{XY} + E[(X^T X)^{-1}] E[(X^T e)] \\\\\n",
        "& = \\mathcal{N}(A_{XY}, E[(X^T X)^{-1}] E[(X^T \\sigma_Y^2)]\n",
        "\\end{align}\n",
        "\n",
        "This then becomes, on the diagonal:\n",
        "$$ \\Delta A_{XY} = \\Sigma_{XX}^{-1}  $$ FIXME\n",
        "\n",
        "Somehow LinRegBlue claims that the uncertainty on $A_{XY}$ is given by\n",
        "$$ \\Delta A_{XY} = \\sigma_X^{-1} R_{XX}^{-1} \\sigma_{Y'} / \\sqrt{ndof} $$\n",
        "\n",
        "FIXME: I think there should be an entire $M \\times M$ covariance matrix for the $M$ sensitivities in $A_{XY}$ for each of the $N$ dependent variables $Y$."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JnI0Om1IXUAO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Independent covariances and standard deviations\n",
        "Sigma_XX = cov[:N,:N]\n",
        "Sigma_X = np.diag(np.sqrt(np.diag(Sigma_XX)))\n",
        "Sigma_XX_Inv = np.linalg.inv(Sigma_XX)\n",
        "Sigma_X_Inv = np.linalg.inv(Sigma_X)\n",
        "R_XX = corr[:N,:N]\n",
        "R_XX_Inv = np.linalg.inv(R_XX)\n",
        "np.testing.assert_allclose(Sigma_X @ R_XX @ Sigma_X, Sigma_XX)\n",
        "\n",
        "# Dependent covariances and standard deviations\n",
        "Sigma_YY = cov[N:,N:]\n",
        "Sigma_Y = np.diag(np.sqrt(np.diag(Sigma_YY)))\n",
        "Sigma_Y_Inv = np.linalg.inv(Sigma_Y)\n",
        "R_YY = corr[N:,N:]\n",
        "np.testing.assert_allclose(Sigma_Y @ R_YY @ Sigma_Y, Sigma_YY)\n",
        "\n",
        "# Cross correlations\n",
        "Sigma_XY = cov[:N,N:]\n",
        "R_XY = corr[:N,N:]\n",
        "np.testing.assert_allclose(Sigma_X @ R_XY @ Sigma_Y, Sigma_XY)\n",
        "\n",
        "# Sensitivities\n",
        "A_XY = Sigma_XX_Inv @ Sigma_XY\n",
        "A_YX = A_XY.T\n",
        "\n",
        "# Uncertainties\n",
        "Sigma_YpYp = Sigma_Y @ Sigma_Y + A_YX @ Sigma_XX @ A_XY - 2 * A_YX @ Sigma_XY\n",
        "sigma_Yp = np.sqrt(np.diag(Sigma_YpYp))\n",
        "ndof = n - N - 1\n",
        "# FIXME\n",
        "dA_XY = np.outer(np.sqrt(np.diag(R_XX_Inv)) * np.diag(Sigma_X_Inv), 1 / (sigma_Yp * np.sqrt(ndof)))\n",
        "dA_YX = dA_XY.T\n",
        "\n",
        "# Plot sensitivities\n",
        "fig, ax = plt.subplots(1,4,sharey = True)\n",
        "ax[0].matshow(Sigma_XY.T)\n",
        "ax[0].set_xlabel(\"independent\")\n",
        "ax[0].set_ylabel(\"dependent\")\n",
        "ax[0].set_title(\"Cov\")\n",
        "ax[1].matshow(A_YX)\n",
        "ax[1].set_xlabel(\"independent\")\n",
        "ax[1].set_title(\"A\")\n",
        "ax[2].matshow(dA_YX)\n",
        "ax[2].set_xlabel(\"independent\")\n",
        "ax[2].set_title(\"$\\\\Delta$A\")\n",
        "ax[3].matshow(dA_YX / A_YX)\n",
        "ax[3].set_xlabel(\"independent\")\n",
        "ax[3].set_title(\"$\\\\Delta$A / A\")\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wpgKSnq4ulQA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setup correction on copy\n",
        "XY_c = XY.copy()\n",
        "X_c = XY_c[:N]\n",
        "Y_c = XY_c[N:]\n",
        "\n",
        "# Apply correction\n",
        "Y_c -= A_YX @ X_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3d2PG395ulFg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate correlations\n",
        "cov_c = np.cov(XY_c)\n",
        "corr_c = np.corrcoef(XY_c)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(1,2)\n",
        "ax[0].matshow(cov_c, cmap = \"hot_r\")\n",
        "ax[1].matshow(corr_c, cmap = \"hot_r\")\n",
        "\n",
        "# assert zero blocks (atol since rtol useless when second argument is zero)\n",
        "np.testing.assert_allclose(cov_c[:N,N:], 0, atol = 1e-7)\n",
        "np.testing.assert_allclose(cov_c[N:,:N], 0, atol = 1e-7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CoJZZPTymS1y",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "means_c = np.mean(XY_c, axis = 1)\n",
        "vars_c = np.var(XY_c, axis = 1)\n",
        "dmeans_c = np.sqrt(vars_c) / np.sqrt(n)\n",
        "\n",
        "# Plot means and variances\n",
        "fig,ax = plt.subplots(1,2)\n",
        "ax[0].errorbar(range(len(means_c)), means_c, dmeans_c, fmt = 'or')\n",
        "ax[0].errorbar(range(len(means)), means, dmeans, fmt = 'ob')\n",
        "ax[1].errorbar(range(len(vars_c)), vars_c, fmt = 'or')\n",
        "ax[1].errorbar(range(len(vars)), vars, fmt = 'ob')\n",
        "plt.show()\n",
        "\n",
        "np.testing.assert_less_or_equal(vars_c[N:], vars[N:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9R2OWQrOGE4q"
      },
      "cell_type": "markdown",
      "source": [
        "#### Testing for Agreement With Matrix Multiplication"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0O_WflAat88g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# correction of the means\n",
        "np.testing.assert_allclose(means[N:] - A_YX @ means[:N], means_c[N:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2gLdO94Pvv3O",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# correction of the new covariances\n",
        "np.testing.assert_allclose(cov[N:,N:] + A_YX @ cov[:N,:N] @ A_XY - 2 * A_YX @ cov[:N,N:], cov_c[N:,N:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KEsop7MsvxMx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# correction of the new variances\n",
        "np.testing.assert_allclose(sigma_Yp**2, vars_c[N:], rtol = 1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}